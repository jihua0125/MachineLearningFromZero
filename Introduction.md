# 数据科学入门之一
## 导论

### 数据与数据科学

在我们进入真正的学习之前，有几个概念需要我们了解一下：什么是数据，什么又是数据科学？
数据是可以定义为一组对某种事物在某一尺度的描述。譬如图像是对事物的视觉描述，文本是对事物的语言描述，语音是对事物的听觉描述。从某种意义上讲，我们所见到的一切都是数据。
根据我们描述事物的不同性质，我们可以把数据分为三种不同类别：
- **连续数据**：如身高、体重等
- **类别数据**：如性别、职业等
- **序数数据**：如排名、级别等

*注意*： 序列数据区别于类别数据，类别数据中各个类别是并列关系，譬如性别中男女并不存在先后关系；而序列数据如排名，则存在明确的优劣关系。
在统计学上，连续数据对应的是连续型变量，而后两者则对应的是离散型变量。当然以上都是量化数据或者说结构化数据，对于非量化数据而言（譬如文本、图像等），我们需要将其转化为可以被量化的数据。比如在自然语言处理领域，对于文本信息常用的量化方式就有词袋（Bag-of-words）模型和词向量（word vector）模型。这些将在以后进一步的讲解。

理解了数据，我们就可以“望文生义”了：数据科学就是研究数据的科学。[维基百科](https://en.wikipedia.org/wiki/Data_science)上给出的定义是：数据科学是一门通过科学方法、过程、系统来从不同形式的（结构化或非结构化）数据中获得知识和信息的交叉学科。从这个角度来讲，数据分析和数据挖掘都可以认为是数据科学的子集，因此在本文中，我都使用数据科学来指代其他近义词。

在实际应用中，数据科学更多的被应用在决策领域，换言之，我们希望利用旧有的数据来帮助预测新的数据。一个典型的应用数据科学帮助决策的项目可以包括以下几个部分：
1. **确定问题**
2. **数据收集**
3. **数据清洗**
4. **数据探索**
5. **构建模型**
6. **模型评估**
7. **得出结论**

表面看来，数据科学的方法和传统方法解决问题并无太大不同，但是其中还是有两点不太一样。一是 **数据量大**，很多领域（如互联网，金融等）使用的数据是以百万、十亿甚至兆来计算；而巨大的数据也引出了第二点不同，即 **使用机器学习方法** 建模。与传统方法不同的是，数据科学一般不依赖人对数据进行分析，而是利用计算机来对数据进行建模，利用一些 **机器学习算法** 来提取数据中的规律，从而进行后续分析。下面我们以一个案例来讲解这个流程。

### 案例分析
> *“I made the first one, I said, 'Let me see if I can make two.'*
>  *I made the second one, I said, 'Let me see if I can make three.'*
> *I MADE THE THIRD ONE, I SAID, 'I’VE GOT A RHYTHM GOING.'”*

> -Kobe Bryant, on setting NBA single-game 3-point record

以笔者做过的一个[简单项目](https://jihua0125.github.io/)为例。这里我们的问题是“篮球中是不是真的有‘手热’的情况”。 这个问题比较抽象，因此我们需要具体定义一下问题。‘手热’可以理解为连续进球，我们可以不同方法定义手热，比如：
- 上一个球投进了，这一个球就更可能进；
- 前`N`个球连续投进，那么这个球更可能进；
- 前`N`次出手至少进了`M`个球，那么这个球更有可能进；
- ...
我们可以对这些假设一一进行检验。

在确立了具体问题之后，如果没有准备好的数据，我们需要收集相关数据。一般而言，我们可以使用互联网上的公开数据，常见方法有：
- 使用`API`: 如我们的项目使用了`NBA`[官方网站](http://stats.nba.com/)统计的数据；
- 使用开源数据包：在一些数据科学相关的网站，如[Kaggle](https://www.kaggle.com/)，会提供一些开源数据进行下载，但是使用时请注意相关协议；
- 使用网络爬虫：对于一些不提供`API`或者有其他特殊需要的网站，可以使用[网络爬虫](https://baike.baidu.com/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB)的方法获取数据。

*注释*：`API`是一些网站或应用提供的接口，可以使用其定义好的方法调用或获取资源，参见[百度百科](https://baike.baidu.com/item/api/10154)。

从网络上获取的数据可能会存在各种问题，譬如格式不统一，某些数据缺失等。因此我们需要将数据整理成能够使用的格式，这一步叫做数据清洗。一般而言，我们希望数据以表格的形式存储。以这个项目为例，我们希望得到的数据是`NxM`的表格，其中每一行是一次投篮出手的信息，这叫做一条 **记录** 或者 **观测值**，每一列是一种 **特征**，如出手位置，分值，是否命中等等。下面是一个简单的例子：

| id    | points | success |
| ------|:------:| -------:|
| 1     | 2      | yes     |
| 2     | 3      | no      |
| 3     |        | no      |

除了格式外，还有常见的问题就是数据缺失，如上图的第三行就缺失了分值数据。处理缺失数据是数据清洗的一个关键步骤，在很多时候会决定后面的建模的好坏。使用的方法有：
- **删除**：如果 **数据量很大** 而 **缺失数据不多且分布随机** 的情况下可以直接删除这些数据，这是比较推荐的方法；
- **插值**：如果 **数据量不大**， 那么每一条记录对我们而言都很重要，我们可以使用插值的办法，如使用平均数或众数替代缺失值，还可以使用`KNN`方法进行插值（以后介绍）；
- **作为特征处理**：在某些情况下，缺失数据可能和其他特征或目标值相关，譬如缺失出手位置的投篮记录全部为未命中的出手，这个时候我们可以建立一个新的二值特征，如果缺少出手位置则此值为1，否则为0；
以上三种办法是比较常用的处理缺失值的方法，三种方法各有优劣：删除法比较简单，但是在数据量小的时候可能导致数据不足而欠拟合；插值法容易引入新的噪声；特征处理在缺失值较多的时候容易导致特征过多。这三种方法需要看情况使用。

在数据清洗时，我们同时会进行数据探索，利用可视化图表来帮助我们理解数据，这一步骤和数据清洗交替进行，例如观察缺失值的分布。这一部分暂时不过多展开，会在随后的教程中进行深入讲解。

在完成了上述步骤之后，我们终于来到了建模这个环节，这里建模环节包括 **模型选择**，**特征选择**，**参数调节** 等等。建模是决定一个数据科学项目的核心内容，如何根据任务选择合适的模型是一个永远也说不完的话题，我们将会在后续的内容系统讲解。在这个项目中，我们选择了对数几率（`logistic`）回归模型。

建立模型之后，我们需要对模型的表现进行评价，不同的模型/任务相对应的评价指标也不同。从整体而言，模型的评价指标包括：
- **准确性**：我们的模型是否准确的描述了我们的数据；
- **泛化性**：我们的模型能否推广到新的数据/任务上；
- **可解释性**： 人类能否解释模型的决策过程；
在后续对机器学习模型讲解中，我们会进一步解释这部分内容。在上一步中，我们选择对数几率回归模型就是希望用其良好的可解释性来判断‘手热’是否能决定命中率。

最后，我们可以通过合适的模型，对新的数据或任务进行预测，得到我们需要的结论。至此，一个典型的数据科学的项目完成了。


### 数据科学的知识结构

根据上面的案例分析，我们可以发现想要成为合格的数据科学家，需要多门知识的组合才可以。数据科学的知识架构可以分为三部分：统计、计算机和行业相关知识（在上个案例中就是篮球知识）。统计本身就是一门刻画数据的学科，合理运用统计知识，可以帮助数据科学家更好的描述数据特征以及设计实验。其中，比较重要的统计学知识是 **概率论、参数估计和假设检验**。计算机领域最重要的当然就是 **机器学习** 了，它可以说是数据科学的灵魂。数据科学在21世纪变得炙手可热，一方面得益于互联网发展带来的数据爆炸式增长，另一方面就得益于机器学习领域的快速发展。最后行业相关知识就是你所从事的行业相关的知识，比如金融、医疗、互联网等等。行业知识只能靠自己的经历去积累，这也是高级数据科学家和刚入门的新手的差距所在。有了以上三方面的知识，我们就可以算是入门数据科学。

### 结语

本篇基础教程先介绍到这里，下一讲将会介绍机器学习的基本概念。
